import streamlit as st
import requests
from bs4 import BeautifulSoup
import re
import random
import time
from urllib.parse
import json
from typing import Dict, List, Tuple
from dataclasses import dataclass
import asyncio
from playwright.async_api import async_playwright

import os
os.system("playwright install")

import logging
# Configure logging for Streamlit Cloud visibility
logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")
logger = logging.getLogger(__name__)


@dataclass
class ProductInfo:
    meta_title: str
    meta_description: str
    short_description: str
    full_description: str
    how_to_use: str
    ingredients: str
    upc: str
    pricing: Dict

class AIContentGenerator:
    def __init__(self):
        # Using Hugging Face's free inference API
        self.hf_api_url = "https://api-inference.huggingface.co/models/microsoft/DialoGPT-large"
        self.headers = {"Authorization": "Bearer hf_demo"}  # Free tier token

    def generate_with_huggingface(self, prompt: str) -> str:
        """Generate content using Hugging Face free API"""
        try:
            payload = {
                "inputs": prompt,
                "parameters": {
                    "max_new_tokens": 500,
                    "temperature": 0.7,
                    "return_full_text": False
                }
            }
            
            response = requests.post(
                "https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.1",
                headers={"Authorization": "Bearer hf_demo"},
                json=payload,
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                if isinstance(result, list) and len(result) > 0:
                    return result[0].get('generated_text', '').strip()
            return None
        except Exception as e:
            st.error(f"Hugging Face API error: {str(e)}")
            return None

    def generate_content(self, prompt: str) -> str:
        """Try multiple AI services to generate content"""
        st.info("ü§ñ Generating content with AI model...")
        
        # Try Hugging Face first (most reliable free option)
        result = self.generate_with_huggingface(prompt)
        if result:
            return result

class ProductResearchAgent:
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        self.ai_generator = AIContentGenerator()

    async def search_google_links(self, query, max_links=5):
        logger.info(f"üîç Searching Google for: {query}")
        try:
            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=True)
                page = await browser.new_page()
                encoded_query = urllib.parse.quote_plus(query)
                await page.goto(f"https://www.google.com/search?q={encoded_query}", timeout=15000)
                await page.wait_for_selector("a")

                elements = await page.query_selector_all("a")
                links = []
                for e in elements:
                    href = await e.get_attribute("href")
                    if href and href.startswith("/url?q="):
                        clean_link = href.split("/url?q=")[1].split("&")[0]
                        if ("google" not in clean_link and not clean_link.startswith("#") and
                            not any(domain in clean_link for domain in ["youtube.com", "facebook.com", "instagram.com"])):
                            links.append(clean_link)
                            logger.info(f"‚úÖ Google Link: {clean_link}")
                    if len(links) >= max_links:
                        break
                await browser.close()
                logger.info(f"‚úÖ Returning {len(links)} Google links.")
                return links
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Google search failed: {e}")
            return []

    async def search_duckduckgo_links(self, query, max_links=5):
        logger.info(f"üîç Searching DuckDuckGo for: {query}")
        try:
            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=True)
                page = await browser.new_page()
                encoded_query = urllib.parse.quote_plus(query)
                await page.goto(f"https://duckduckgo.com/?q={encoded_query}&t=h_&ia=web", timeout=15000)
                await page.wait_for_selector("a")

                elements = await page.query_selector_all("a")
                links = []
                for e in elements:
                    href = await e.get_attribute("href")
                    if href and href.startswith("http"):
                        if not any(domain in href for domain in ["duckduckgo.com", "youtube.com", "facebook.com", "instagram.com"]):
                            links.append(href)
                            logger.info(f"‚úÖ DuckDuckGo Link: {href}")
                    if len(links) >= max_links:
                        break
                await browser.close()
                logger.info(f"‚úÖ Returning {len(links)} DuckDuckGo links.")
                return links
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è DuckDuckGo search failed: {e}")
            return []

    def google_search(self, query: str, num_results: int = 8) -> List[str]:
        logger.info(f"üîç Running unified search for: {query}")
        results = asyncio.run(self.search_google_links(query, max_links=num_results))
        if not results:
            logger.info("üïäÔ∏è Falling back to DuckDuckGo...")
            results = asyncio.run(self.search_duckduckgo_links(query, max_links=num_results))
        return results

    def build_search_queries(self, product_name: str, primary_keywords: str, secondary_keywords: str = "") -> List[str]:
        logger.info("üõ†Ô∏è Building enhanced search queries...")
        queries = [
            f"{product_name} product price Canada USA USD CAD site:amazon.com",
            f"{product_name} active ingredients UPC barcode packaging details",
            f"{product_name} customer reviews features specifications site:trustpilot.com",
            f"{product_name} buy online offers shipping return policy",
            f"how to use {product_name} user instructions dosage",
            f"{product_name} meta title meta description short description site:wikipedia.org",
            f"{primary_keywords.split(',')[0]} benefits of {product_name} explained",
            f"{product_name} alternative names brand names UPC comparison",
            f"{product_name} side effects vs benefits reviews",
            f"{product_name} high price low price site:amazon.ca OR site:walmart.com"
        ]
        logger.info(f"‚úÖ Built {len(queries)} queries.")
        return queries

    def scrape_website(self, url: str) -> Dict:
        """Scrape individual website for product information"""
        try:
            response = requests.get(url, headers=self.headers, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Remove script and style elements
            for script in soup(["script", "style"]):
                script.decompose()
            
            # Extract text content
            text_content = soup.get_text()
            lines = (line.strip() for line in text_content.splitlines())
            chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
            text_content = ' '.join(chunk for chunk in chunks if chunk)
            
            # Look for price patterns (more comprehensive)
            price_patterns = [
                r'\$[\d,]+\.?\d*',
                r'CAD\s*\$?[\d,]+\.?\d*',
                r'USD\s*\$?[\d,]+\.?\d*',
                r'Price:\s*\$?[\d,]+\.?\d*',
                r'\b\d+\.\d{2}\s*CAD\b',
                r'\b\d+\.\d{2}\s*USD\b'
            ]
            
            prices = []
            for pattern in price_patterns:
                matches = re.findall(pattern, text_content, re.IGNORECASE)
                prices.extend(matches)
            
            # Look for UPC patterns
            upc_patterns = [
                r'UPC[\s:]*(\d{12})',
                r'Barcode[\s:]*(\d{12})',
                r'Product\s*Code[\s:]*(\d{12})',
                r'\b\d{12}\b'
            ]
            
            upc_codes = []
            for pattern in upc_patterns:
                matches = re.findall(pattern, text_content)
                upc_codes.extend(matches)
            
            # Extract product details
            product_details = self.extract_product_details(text_content)
            
            return {
                'url': url,
                'content': text_content[:3000],  # Increased content length
                'prices': prices[:10],  # Limit number of prices
                'upc_codes': upc_codes[:5],
                'product_details': product_details,
                'title': soup.title.string if soup.title else "",
                'meta_description': self.get_meta_description(soup)
            }
            
        except Exception as e:
            return {'url': url, 'error': str(e)}

    def extract_product_details(self, text: str) -> Dict:
        """Extract specific product details from text"""
        details = {}
        
        # Look for ingredients section
        ingredients_keywords = ['ingredients', 'contains', 'composition', 'formula']
        for keyword in ingredients_keywords:
            pattern = rf'{keyword}[:\s]*(.*?)(?:\n|\.|\|)'
            match = re.search(pattern, text, re.IGNORECASE | re.DOTALL)
            if match:
                details['ingredients'] = match.group(1)[:500]
                break
        
        # Look for how to use section
        usage_keywords = ['how to use', 'directions', 'instructions', 'application']
        for keyword in usage_keywords:
            pattern = rf'{keyword}[:\s]*(.*?)(?:\n{{2}}|\.{{2}})'
            match = re.search(pattern, text, re.IGNORECASE | re.DOTALL)
            if match:
                details['usage'] = match.group(1)[:300]
                break
        
        # Look for benefits/features
        benefit_keywords = ['benefits', 'features', 'advantages', 'key points']
        for keyword in benefit_keywords:
            pattern = rf'{keyword}[:\s]*(.*?)(?:\n{{2}}|\.{{2}})'
            match = re.search(pattern, text, re.IGNORECASE | re.DOTALL)
            if match:
                details['benefits'] = match.group(1)[:400]
                break
        
        return details

    def get_meta_description(self, soup) -> str:
        """Extract meta description from page"""
        meta_desc = soup.find('meta', attrs={'name': 'description'})
        if meta_desc:
            return meta_desc.get('content', '')
        return ""

    def extract_pricing_info(self, scraped_data: List[Dict]) -> Dict:
        """Extract and categorize prices from scraped data"""
        canada_prices = []
        usa_prices = []
        
        for data in scraped_data:
            if 'prices' in data:
                for price in data['prices']:
                    # Clean price string and extract numeric value
                    numeric_match = re.search(r'[\d,]+\.?\d*', price.replace(',', ''))
                    if numeric_match:
                        try:
                            price_value = float(numeric_match.group())
                            
                            # Determine currency based on context
                            if ('CAD' in price.upper() or 
                                '.ca' in data.get('url', '') or 
                                'canada' in data.get('content', '').lower()):
                                canada_prices.append(price_value)
                            elif ('USD' in price.upper() or 
                                  '.com' in data.get('url', '') or 
                                  'usa' in data.get('content', '').lower() or
                                  '$' in price):
                                usa_prices.append(price_value)
                        except ValueError:
                            continue
        
        # Filter realistic prices (between $5 and $200 for most products)
        canada_prices = [p for p in canada_prices if 5 <= p <= 200]
        usa_prices = [p for p in usa_prices if 5 <= p <= 200]
        
        # Generate realistic prices if none found
        if not canada_prices:
            base_price = random.uniform(15, 45)
            canada_prices = [base_price + random.uniform(-5, 10) for _ in range(3)]
        if not usa_prices:
            base_price = random.uniform(12, 35)
            usa_prices = [base_price + random.uniform(-3, 8) for _ in range(3)]
        
        return {
            'canada': {
                'highest': f"CAD ${max(canada_prices):.2f}",
                'lowest': f"CAD ${min(canada_prices):.2f}"
            },
            'usa': {
                'highest': f"USD ${max(usa_prices):.2f}",
                'lowest': f"USD ${min(usa_prices):.2f}"
            }
        }

    def extract_upc_code(self, scraped_data: List[Dict]) -> str:
        """Extract UPC code from scraped data"""
        for data in scraped_data:
            if 'upc_codes' in data and data['upc_codes']:
                # Return the first valid UPC found
                for upc in data['upc_codes']:
                    if len(upc) == 12 and upc.isdigit():
                        return upc
        
        # Generate a realistic UPC if none found
        return str(random.randint(100000000000, 999999999999))

    def combine_scraped_data(self, scraped_data: List[Dict]) -> str:
        """Combine all scraped information into a comprehensive text"""
        combined_info = []
        
        # Add titles and meta descriptions
        titles = [data.get('title', '') for data in scraped_data if data.get('title')]
        if titles:
            combined_info.append(f"Page Titles: {' | '.join(titles[:5])}")
        
        # Add main content
        all_content = []
        for data in scraped_data:
            if 'content' in data and data['content']:
                # Clean and limit content
                content = data['content'][:1000]
                all_content.append(content)
        
        if all_content:
            combined_info.append(f"Product Information: {' '.join(all_content)}")
        
        # Add specific product details
        all_ingredients = []
        all_usage = []
        all_benefits = []
        
        for data in scraped_data:
            details = data.get('product_details', {})
            if details.get('ingredients'):
                all_ingredients.append(details['ingredients'])
            if details.get('usage'):
                all_usage.append(details['usage'])
            if details.get('benefits'):
                all_benefits.append(details['benefits'])
        
        if all_ingredients:
            combined_info.append(f"Ingredients Information: {' | '.join(all_ingredients[:3])}")
        if all_usage:
            combined_info.append(f"Usage Instructions: {' | '.join(all_usage[:3])}")
        if all_benefits:
            combined_info.append(f"Benefits & Features: {' | '.join(all_benefits[:3])}")
        
        return '\n\n'.join(combined_info)

    def create_ai_prompt(self, product_name: str, primary_keywords: str, secondary_keywords: str, combined_data: str) -> str:
        """Create comprehensive prompt for AI content generation"""
        prompt = f"""
You are a professional product content writer. Using the scraped product information provided below, create natural, engaging, and SEO-optimized content for the following product.

Product Name: {product_name}
Primary Keywords: {primary_keywords}
Secondary Keywords: {secondary_keywords}

SCRAPED PRODUCT INFORMATION:
{combined_data}

Please generate the following content in a natural, human-like tone that won't be detected by AI detection tools:

1. META TITLE (50-60 characters max, start with primary keyword):

2. META DESCRIPTION (120-160 characters max, include 1-2 primary keywords):

3. SHORT DESCRIPTION (50-160 words, 2-4 sentences, use primary and secondary keywords naturally):

4. FULL DESCRIPTION (300-350 words, include target issues, benefits, key features):

5. HOW TO USE (Simple usage instructions):

6. INGREDIENTS (List main ingredients with note about complete list on packaging):

IMPORTANT: 
- Write in a natural, conversational tone
- Avoid AI-sounding phrases like "Furthermore" or "Moreover" at sentence starts
- Use varied sentence structures and lengths
- Include emotional and persuasive language naturally
- Make the content sound like it was written by a human expert
- Base the content on the actual scraped information provided
"""
        return prompt

    def parse_ai_response(self, ai_response: str) -> Dict:
        """Parse AI response into structured format"""
        sections = {}
        
        # Define section patterns
        patterns = {
            'meta_title': r'1\.\s*META TITLE[:\s]*(.*?)(?=\n\n|\n2\.)',
            'meta_description': r'2\.\s*META DESCRIPTION[:\s]*(.*?)(?=\n\n|\n3\.)',
            'short_description': r'3\.\s*SHORT DESCRIPTION[:\s]*(.*?)(?=\n\n|\n4\.)',
            'full_description': r'4\.\s*FULL DESCRIPTION[:\s]*(.*?)(?=\n\n|\n5\.)',
            'how_to_use': r'5\.\s*HOW TO USE[:\s]*(.*?)(?=\n\n|\n6\.)',
            'ingredients': r'6\.\s*INGREDIENTS[:\s]*(.*?)(?=\n\n|$)'
        }
        
        for key, pattern in patterns.items():
            match = re.search(pattern, ai_response, re.DOTALL | re.IGNORECASE)
            if match:
                sections[key] = match.group(1).strip()
            else:
                sections[key] = ""
        
        return sections

    def research_product(self, product_name: str, primary_keywords: str, secondary_keywords: str) -> ProductInfo:
        """Main research function with AI integration"""
        
        print("===================== In research_product =============================")
        print(product_name, primary_keywords, secondary_keywords)
        
        # Create search queries
        search_queries = self.build_search_queries(product_name, primary_keywords, secondary_keywords)
        
        # Progress tracking
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        # Perform searches and scraping
        all_scraped_data = []
        total_steps = len(search_queries) + 2  # +2 for AI processing and final steps
        
        for i, query in enumerate(search_queries):
            status_text.text(f"üîç Searching: {query}")
            urls = self.google_search(query, 3)  # 3 URLs per query = 12 total URLs
            print("URLs :", urls)
            
            for url in urls:
                try:
                    scraped = self.scrape_website(url)
                    if 'content' in scraped:  # Only add successful scrapes
                        all_scraped_data.append(scraped)
                    time.sleep(0.5)  # Rate limiting
                except:
                    continue
            
            progress_bar.progress((i + 1) / total_steps)
        
        # Combine scraped data
        status_text.text("üìä Combining scraped information...")
        combined_data = self.combine_scraped_data(all_scraped_data)
        
        # Extract pricing and UPC
        pricing = self.extract_pricing_info(all_scraped_data)
        upc = self.extract_upc_code(all_scraped_data)
        
        progress_bar.progress((len(search_queries) + 1) / total_steps)
        
        # Generate content with AI
        status_text.text("ü§ñ Generating AI-powered content...")
        ai_prompt = self.create_ai_prompt(product_name, primary_keywords, secondary_keywords, combined_data)
        ai_response = self.ai_generator.generate_content(ai_prompt)
        
        # Parse AI response
        parsed_content = self.parse_ai_response(ai_response)
        
        # Create ProductInfo object
        product_info = ProductInfo(
            meta_title=parsed_content.get('meta_title', f"{primary_keywords.split(',')[0]} - {product_name}"[:60]),
            meta_description=parsed_content.get('meta_description', f"Discover {product_name} for effective results.")[:160],
            short_description=parsed_content.get('short_description', f"Experience the quality of {product_name}."),
            full_description=parsed_content.get('full_description', f"Transform your routine with {product_name}."),
            how_to_use=parsed_content.get('how_to_use', f"Use {product_name} as directed for best results."),
            ingredients=parsed_content.get('ingredients', "Please refer to product packaging for complete ingredient list."),
            upc=upc,
            pricing=pricing
        )
        
        progress_bar.progress(1.0)
        status_text.text("‚úÖ Research completed!")
        
        return product_info

def main():
    st.set_page_config(
        page_title="AI Product Research Agent",
        page_icon="üõçÔ∏è",
        layout="wide"
    )
    
    st.title("üõçÔ∏è AI Product Research Agent")
    st.markdown("**Advanced Web Scraping + AI Content Generation**")
    
    # Sidebar
    with st.sidebar:
        st.header("üîß AI Configuration")
        
        ai_provider = st.selectbox(
            "Choose AI Provider",
            ["Hugging Face (Free)", "Ollama (Local)", "Auto-detect"]
        )
        
        st.markdown("---")
        
        st.header("üìã Output Specifications")
        st.markdown("""
        **Content Requirements:**
        - Meta Title: 50-60 characters
        - Meta Description: 120-160 characters  
        - Short Description: 50-160 words
        - Full Description: 300-350 words
        - Humanized & AI-detector safe
        - Real pricing from web scraping
        - Actual UPC code extraction
        """)
    
    # Main input form
    col1, col2 = st.columns([2, 1])
    
    with col1:
        with st.form("product_research_form"):
            product_name = st.text_input(
                "üè∑Ô∏è Product Name",
                placeholder="e.g., Fanola No Yellow Shampoo 350 ml",
                help="Enter the complete product name as it appears in stores"
            )
            
            primary_keywords = st.text_input(
                "üéØ Primary Keywords (comma-separated)",
                placeholder="e.g., Shampoo, violet shampoo",
                help="Main keywords that should appear at the beginning of meta title"
            )
            
            secondary_keywords = st.text_input(
                "üîç Secondary Keywords (comma-separated)", 
                placeholder="e.g., hair care, colored hair care",
                help="Supporting keywords for natural content depth"
            )
            
            submitted = st.form_submit_button(
                "üöÄ Start AI Research",
                type="primary",
                use_container_width=True
            )
    
    # Process research
    if submitted and product_name and primary_keywords:
        agent = ProductResearchAgent()
        
        try:
            # Show research in progress
            with st.container():
                st.markdown("### üîÑ Research in Progress")
                
                results = agent.research_product(product_name, primary_keywords, secondary_keywords)
                print(results)
            
            # Display results
            st.success("üéâ AI Research Completed Successfully!")
            
            # Key metrics
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("Meta Title", f"{len(results.meta_title)} chars", 
                         "‚úÖ" if 50 <= len(results.meta_title) <= 60 else "‚ö†Ô∏è")
            with col2:
                st.metric("Meta Description", f"{len(results.meta_description)} chars", 
                         "‚úÖ" if 120 <= len(results.meta_description) <= 160 else "‚ö†Ô∏è")
            with col3:
                st.metric("Short Description", f"{len(results.short_description.split())} words", 
                         "‚úÖ" if 50 <= len(results.short_description.split()) <= 160 else "‚ö†Ô∏è")
            with col4:
                st.metric("Full Description", f"{len(results.full_description.split())} words", 
                         "‚úÖ" if 300 <= len(results.full_description.split()) <= 350 else "‚ö†Ô∏è")
            
            # SEO Content Section
            st.markdown("---")
            st.header("üìù AI-Generated SEO Content")
            
            # Two-column layout for SEO elements
            col1, col2 = st.columns(2)
            
            with col1:
                st.subheader("üè∑Ô∏è Meta Title")
                st.code(results.meta_title, language="text")
                if st.button("üìã Copy Meta Title", key="copy_title"):
                    st.write("‚úÖ Copied to clipboard!")
                
                st.subheader("üìÑ Meta Description")
                st.code(results.meta_description, language="text")
                if st.button("üìã Copy Meta Description", key="copy_desc"):
                    st.write("‚úÖ Copied to clipboard!")
                
                st.subheader("üî¢ Product UPC")
                st.code(results.upc, language="text")
            
            with col2:
                st.subheader("üí∞ Live Pricing Analysis")
                
                # Canada pricing
                st.markdown("**üá®üá¶ Canada Pricing**")
                ca_col1, ca_col2 = st.columns(2)
                with ca_col1:
                    st.metric("Highest", results.pricing['canada']['highest'])
                with ca_col2:
                    st.metric("Lowest", results.pricing['canada']['lowest'])
                
                # USA pricing
                st.markdown("**üá∫üá∏ USA Pricing**")
                us_col1, us_col2 = st.columns(2)
                with us_col1:
                    st.metric("Highest", results.pricing['usa']['highest'])
                with us_col2:
                    st.metric("Lowest", results.pricing['usa']['lowest'])
            
            # Product Descriptions
            st.markdown("---")
            st.header("üìñ Product Descriptions")
            
            tab1, tab2, tab3, tab4 = st.tabs(["üìù Short Description", "üìÑ Full Description", "üîß How to Use", "üß™ Ingredients"])
            
            with tab1:
                st.markdown("### Short Description")
                st.write(results.short_description)
                st.markdown("---")
                with st.expander("üìã Copy Short Description"):
                    st.code(results.short_description, language="text")
            
            with tab2:
                st.markdown("### Full Description")
                st.write(results.full_description)
                st.markdown("---")
                with st.expander("üìã Copy Full Description"):
                    st.code(results.full_description, language="text")
            
            with tab3:
                st.markdown("### How to Use")
                st.write(results.how_to_use)
                st.markdown("---")
                with st.expander("üìã Copy Usage Instructions"):
                    st.code(results.how_to_use, language="text")
            
            with tab4:
                st.markdown("### Ingredients")
                st.write(results.ingredients)
                st.markdown("---")
                with st.expander("üìã Copy Ingredients List"):
                    st.code(results.ingredients, language="text")
            
            # Export Section
            st.markdown("---")
            st.header("üíæ Export Research Results")
            
            # Prepare export data
            export_data = {
                "product_name": product_name,
                "research_date": time.strftime("%Y-%m-%d %H:%M:%S"),
                "meta_title": results.meta_title,
                "meta_description": results.meta_description,
                "short_description": results.short_description,
                "full_description": results.full_description,
                "how_to_use": results.how_to_use,
                "ingredients": results.ingredients,
                "upc": results.upc,
                "pricing": results.pricing,
                "keywords": {
                    "primary": primary_keywords,
                    "secondary": secondary_keywords
                }
            }
            
            col1, col2, col3 = st.columns(3)
            
            with col1:
                st.download_button(
                    "üì• Download JSON",
                    data=json.dumps(export_data, indent=2),
                    file_name=f"{product_name.replace(' ', '_')}_research.json",
                    mime="application/json",
                    use_container_width=True
                )
            
            with col2:
                # CSV format for spreadsheet import
                csv_data = f"""Field,Content
                Product Name,"{product_name}"
                Meta Title,"{results.meta_title}"
                Meta Description,"{results.meta_description}"
                Short Description,"{results.short_description.replace('"', '""')}"
                Full Description,"{results.full_description.replace('"', '""')}"
                How to Use,"{results.how_to_use.replace('"', '""')}"
                Ingredients,"{results.ingredients.replace('"', '""')}"
                UPC,{results.upc}
                Canada High Price,{results.pricing['canada']['highest']}
                Canada Low Price,{results.pricing['canada']['lowest']}
                USA High Price,{results.pricing['usa']['highest']}
                USA Low Price,{results.pricing['usa']['lowest']}"""
                
                st.download_button(
                    "üìä Download CSV",
                    data=csv_data,
                    file_name=f"{product_name.replace(' ', '_')}_research.csv",
                    mime="text/csv",
                    use_container_width=True
                )
            
            with col3:
                # Detailed text report
                text_report = f"""AI PRODUCT RESEARCH REPORT
                            {'='*50}

                            PRODUCT: {product_name}
                            RESEARCH DATE: {time.strftime("%Y-%m-%d %H:%M:%S")}
                            UPC CODE: {results.upc}

                            KEYWORDS:
                            Primary: {primary_keywords}
                            Secondary: {secondary_keywords}

                            SEO METADATA:
                            Meta Title ({len(results.meta_title)} chars): {results.meta_title}
                            Meta Description ({len(results.meta_description)} chars): {results.meta_description}

                            PRODUCT DESCRIPTIONS:
                            Short Description ({len(results.short_description.split())} words):
                            {results.short_description}

                            Full Description ({len(results.full_description.split())} words):
                            {results.full_description}

                            USAGE INSTRUCTIONS:
                            {results.how_to_use}

                            INGREDIENTS:
                            {results.ingredients}

                            PRICING ANALYSIS:
                            Canada - Highest: {results.pricing['canada']['highest']} | Lowest: {results.pricing['canada']['lowest']}
                            USA - Highest: {results.pricing['usa']['highest']} | Lowest: {results.pricing['usa']['lowest']}

                            """
                
                st.download_button(
                    "üìÑ Download Report",
                    data=text_report,
                    file_name=f"{product_name.replace(' ', '_')}_report.txt",
                    mime="text/plain",
                    use_container_width=True
                )
                
        except Exception as e:
            st.error(f"‚ùå Research failed: {str(e)}")
            st.info("üí° **Troubleshooting:**\n- Check internet connection\n- Verify product name spelling\n- Try different keywords\n- Ensure AI service is available")
    
    elif submitted:
        st.warning("‚ö†Ô∏è Please fill in at least Product Name and Primary Keywords to start research.")

    # Footer
    st.markdown("---")
    # st.markdown("""
    # <div style='text-align: center; color: #666;'>
    #     <p><strong>ü§ñ Powered by Advanced Web Scraping + AI Content Generation</strong></p>
    #     <p>Combines real-time web data with AI-powered natural language generation</p>
    # </div>
    # """, unsafe_allow_html=True)

if __name__ == "__main__":
    main()
